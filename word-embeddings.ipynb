{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Word Embeddings\n",
    "\n",
    "## ¬øC√≥mo representamos palabras, oraciones y significados en NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## ¬øQu√© es una palabra?\n",
    "\n",
    "\n",
    "![](img/words.png)\n",
    "\n",
    "Cuando hablamos de palabras, podemos distinguir dos conceptos diferentes:\n",
    "\n",
    "- **ocurrencia** (*token*) se refiere a una observaci√≥n de una palabra en una cadena de texto. \n",
    "\n",
    "    Como hemos visto, en algunas lenguas es m√°s o menos complejo identificar los l√≠mites de las palabras, pero en la mayor√≠a de las lenguas occidentales y de nuestro entorno se utilizan espacios y otros signos de puntuaci√≥n para delimitar las palabras.\n",
    "\n",
    "- **tipo** (*type*) es la representaci√≥n abstracta de una palabra. Cad **ocurrencia** pertenece a un **tipo** de palabra. Cuando contamos la frecuencia de las palabras de un *corpus* o colecci√≥n de textos, lo que hacemos es contar el n√∫mero de ocurrencias que tiene cada tipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "texts = [\n",
    "    \"\"\"No hubo sorpresa en Bruselas. 621 votos a favor, 49 en contra (los 'remainers' brit√°nicos entre ellos) y 13 abstenciones.\"\"\",\n",
    "    \"\"\"'The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced \"no evidence\" that any irregularities took place.'\"\"\",\n",
    "    \"\"\"Áí∞Â§™Âπ≥Ê¥ãÈÄ†Â±±Â∏Ø„Å´Â±û„Åô„ÇãÂ∞è„Çπ„É≥„ÉÄÂàóÂ≥∂„ÅÆË•øÁ´Ø„Å´‰ΩçÁΩÆ„Åó„Å¶„ÅÑ„Çã„ÄÇ\"\"\",\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"\"\"üéâ¬°#SORTEO! Gana una tostadora YummyToast Double. üéÅ \n",
    "‚ñ™Ô∏èS√≠guenos. \n",
    "‚ñ™Ô∏èComenta mencionando a 2 amigos junto a #Cecotec.\n",
    "Tienes hasta el 9 de febrero para participar. El regalo se sortear√° aleatoriamente entre los participantes. ¬°Mucha suerte!.\"\"\",\n",
    "    \"\"\"we play for y‚Äôall üèÄ‚ÄºÔ∏èüñ§ https://t.co/sd12vW93 #MambaMentality\"\"\",\n",
    "]\n",
    "\n",
    "for tweet in tweets:\n",
    "    print(word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "for text in texts:\n",
    "    print(tokenizer.tokenize(text))\n",
    "\n",
    "for tweet in tweets:\n",
    "    print(tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "for text in texts + tweets:\n",
    "    print(tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Veamos qu√© tipo de tokenizaci√≥n se prefiere cuando son humanos los que segmentan las palabras: el [corpus de Brown](https://en.wikipedia.org/wiki/Brown_Corpus) en ingl√©s, o [Ancora](http://clic.ub.edu/corpus/es) en espa√±ol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "brown_sents = brown.tagged_sents(categories=\"news\")\n",
    "for sentence in brown_sents[:3]:\n",
    "    print([token for token, _tag in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import cess_esp\n",
    "\n",
    "ancora_sents = cess_esp.tagged_sents()\n",
    "for sentence in ancora_sents[:3]:\n",
    "    print([token for token, _tag in sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Representaciones discretas\n",
    "\n",
    "A partir de aqu√≠ vamos a asumir que tenemos solucionado el proceso de tokenizaci√≥n e identificaci√≥n de lo que es una palabra. ¬øC√≥mo continuamos?\n",
    "\n",
    "La manera m√°s sencilla de representar una palabra es una cadena, es decir, como una secuencia ordenada de caracteres. Esto es c√≥modo, pero implica dos cosas:\n",
    "\n",
    "- La cantidad de memoria que ocupa cada cada palabra var√≠a en funci√≥n de la longitud :-/\n",
    "\n",
    "- Comprobar si dos palabras son id√©nticas es un proceso lento :-(\n",
    "\n",
    "Otra opci√≥n alternativa consiste en representar las palabras como n√∫meros enteros, de manera que a cada palabra se le asigna de manera m√°s o menos arbitraria un n√∫mero entero positivo.\n",
    "\n",
    "![](img/words-indexes.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\"comp.windows.x\", \"rec.sport.baseball\", \"sci.space\", \"talk.religion.misc\"]\n",
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset=\"train\", categories=categories, remove=remove\n",
    ")\n",
    "\n",
    "newsgroups_train.filenames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "for doc in newsgroups_train.data[:3]:\n",
    "    print(doc[:300])\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "for doc in newsgroups_train.data[-3:]:\n",
    "    print(doc[:300])\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "tweets_tokens = []\n",
    "tweets_tokens.extend([tokenize(tweet) for tweet in tweets][0])\n",
    "print(tweets_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "tokens_int = label_encoder.fit_transform(tweets_tokens)\n",
    "\n",
    "token2int = dict(zip(tweets_tokens, tokens_int))\n",
    "print(token2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Este tipo de representaci√≥n se caracteriza porque:\n",
    "\n",
    "- Todos las palabras ocupan la cantidad de memoria.\n",
    "\n",
    "- Comprobar si dos cadenas son la misma palabra es r√°pido :-)\n",
    "\n",
    "- Estos identificadores arbitrarios no significan nada :-(\n",
    "\n",
    "- No hay manera de relacionar palabras similares atendiendo a su identificador :-("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# aplanamos los tokens_int\n",
    "tokens_int = tokens_int.reshape(len(tokens_int), 1)\n",
    "onehot_tokens = onehot_encoder.fit_transform(tokens_int)\n",
    "\n",
    "print(onehot_tokens)\n",
    "\n",
    "print(onehot_tokens[token2int[\"suerte\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "onehot_tokens = to_categorical(tokens_int)\n",
    "\n",
    "print(onehot_tokens)\n",
    "print(onehot_tokens[token2int[\"suerte\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizar colecciones de documentos es una pr√°ctica habitual. Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "vectors = vectorizer.fit_transform(\n",
    "    newsgroups_train.data\n",
    ").todense()  # (documents, vocab)\n",
    "\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Hemos convertido la collecci√≥n de documentos en una matriz de datos donde los documentos se representan como vectores de enteros.\n",
    "\n",
    "![](img/vectorized-docs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(vocab[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(vocab[20000:20050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(vocab[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Palabras como vectores\n",
    "\n",
    "La verdad es que hoy en d√≠a nunca se representamos las palabras como elementos discretos principalmente por un motivo: muchas veces no necesitaremos comprobar no si dos palabras son iguales, como mencion√°bamos antes, pero s√≠ si dos palabras son *similares*. Calcular la **similitud** entre dos pares de palabras/oraciones/documentos es crucial para muchas tareas de NLP.\n",
    "\n",
    "### ¬øQu√© significa **similitud**?\n",
    "\n",
    "La idea de similitud implica que dos palabras tienen alg√∫n tipo de relaci√≥n desde el punto de vista del significado, no necesariamente de sinonimia. Que dos palabras sean similares no implica que sean sin√≥nimas e intercambiables en cualquier situaci√≥n. \n",
    "\n",
    "- Dos adjetivos aparentemente contrarios (p. ej. *blanco* y *negro*) son similares porque se pueden aplicar a los mismos objetos. \n",
    "- Un t√©rmino general y otro m√°s espec√≠fico (p. ej. *perro* y *caniche*) son similares tambi√©n. \n",
    "- Una parte constitutiva y el todo (p. ej. *dedo* y *mano*) son similares.\n",
    "\n",
    "Hist√≥ricamente, ha habido distintos intentos de codificar de manera expl√≠cita estas relaciones de similitud a mano. Desde el punto de vista Sem√°ntica, el ejemplo m√°s famoso es [Wordnet](https://wordnet.princeton.edu/), una base de datos l√©xica que almacena palabras, sus significados y las relaciones sem√°nticas que se establecen entre ellas de manera jer√°rquica. Otras partes de la ling√º√≠sitica, como la Sintaxis, estudia la estructura del lenguaje y agrupa las palabras similares bajo clases de palabras o categor√≠as como *nombre*, *verbo*, *adjetivo*, etc. \n",
    "\n",
    "M√°s rencientemente, se ha desarrollado una tendencia (hasta convertirse en dominante) consistente en extraer este tipo de relaciones de manera autom√°tica, a trav√©s del procesamiento de ingentes colecciones de textos y el an√°lisis de cada palabra en su contexto de aparici√≥n.\n",
    "\n",
    "A partir de cualquiera de estas dos vertientes, o m√°s bien, combinando ambas, podemos llegar a la idea de representar una palabra como un vector. Y podemos elegir la dimensionalidad que mejor se ajuste a nuestros intereses:\n",
    "\n",
    "\n",
    "- ‚Ä¢ Each word type may be given its own dimension, and assigned 1 in that dimension (while all other\n",
    "words get 0 in that dimension). Using dimensions only in this way, and no other, is essentially\n",
    "equivalent to integerizing the words; it is known as a ‚Äúone hot‚Äù representation, because each word\n",
    "type‚Äôs vector has a single 1 (‚Äúhot‚Äù) and is otherwise 0.\n",
    "‚Ä¢ For a collection of word types that belong to a known class (e.g., days of the week), we can use a\n",
    "dimension that is given binary values. Word types that are members of the class get assigned 1 in this\n",
    "dimension, and other words get 0.\n",
    "‚Ä¢ For word types that are variants of the same underlying root, we can similarly use a dimension to\n",
    "place them in a class. For example, in this dimension, know, known, knew, and knows would all get\n",
    "assigned 1, and words that are not forms of know get 0.\n",
    "‚Ä¢ More loosely, we can use surface attributes to ‚Äútie together‚Äù word types that look similar; examples\n",
    "include capitalization patterns, lengths, and the presence of a digit.\n",
    "‚Ä¢ If word types‚Äô meanings can be mapped to magnitudes, we might allocate dimensions to try to capture\n",
    "these. For example, in a dimension we choose to associate with ‚Äútypical weight‚Äù elephant might get\n",
    "12,000 while cat might get 9. Of course, it‚Äôs not entirely clear what value to give purple or throw in\n",
    "this dimension.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Cuando tenemos un mapeo como este entre palabras y entereos podemos representar cada palabra como un vector de $n$ dimensiones, donde $n$ es el tama√±o de vocabulario que manejamos. Estos vectores contendr√°n muchos $0$ y un √∫nico $1$, en la posici√≥n que coincida con el √≠ndice de la palabra en el vocabulario.\n",
    "\n",
    "![](img/one-hot-vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Palabras como vectores distribucionales\n",
    "\n",
    "> ‚ÄúYou shall know a word by the company it keeps.‚Äù\n",
    "> ‚Äî John R. Firth (1957)\n",
    ">\n",
    ">‚ÄúThe meaning of a word is its use in the language (‚Ä¶) One cannot guess how a word functions. One has to look at its use, and learn from that.‚Äù\n",
    ">‚Äî Ludwig Wittgenstein (1953)\n",
    "\n",
    "\n",
    "La idea de que podemos analizar el uso de las palabras para deducir sus significado es una idea fundamental en sem√°ntica distribucional: la hip√≥tesis distribucional. \n",
    "\n",
    "Esta idea inspira muchos algoritmos para aprender repesentaciones num√©ricas de las palanbras --> *word embeddings*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Palabras como vectores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
