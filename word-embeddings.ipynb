{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Word Embeddings\n",
    "\n",
    "## Â¿CÃ³mo representamos palabras, oraciones y significados en NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Â¿QuÃ© es una palabra?\n",
    "\n",
    "\n",
    "![](img/words.png)\n",
    "\n",
    "Cuando hablamos de palabras, podemos distinguir dos conceptos diferentes:\n",
    "\n",
    "- **ocurrencia** (*token*) se refiere a una observaciÃ³n de una palabra en una cadena de texto. \n",
    "\n",
    "    Como hemos visto, en algunas lenguas es mÃ¡s o menos complejo identificar los lÃ­mites de las palabras, pero en la mayorÃ­a de las lenguas occidentales y de nuestro entorno se utilizan espacios y otros signos de puntuaciÃ³n para delimitar las palabras.\n",
    "\n",
    "- **tipo** (*type*) es la representaciÃ³n abstracta de una palabra. Cad **ocurrencia** pertenece a un **tipo** de palabra. Cuando contamos la frecuencia de las palabras de un *corpus* o colecciÃ³n de textos, lo que hacemos es contar el nÃºmero de ocurrencias que tiene cada tipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "texts = [\n",
    "    \"\"\"No hubo sorpresa en Bruselas. 621 votos a favor, 49 en contra (los 'remainers' britÃ¡nicos entre ellos) y 13 abstenciones.\"\"\",\n",
    "    \"\"\"'The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced \"no evidence\" that any irregularities took place.'\"\"\",\n",
    "    \"\"\"ç’°å¤ªå¹³æ´‹é€ å±±å¸¯ã«å±žã™ã‚‹å°ã‚¹ãƒ³ãƒ€åˆ—å³¶ã®è¥¿ç«¯ã«ä½ç½®ã—ã¦ã„ã‚‹ã€‚\"\"\",\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"\"\"ðŸŽ‰Â¡#SORTEO! Gana una tostadora YummyToast Double. ðŸŽ \n",
    "â–ªï¸SÃ­guenos. \n",
    "â–ªï¸Comenta mencionando a 2 amigos junto a #Cecotec.\n",
    "Tienes hasta el 9 de febrero para participar. El regalo se sortearÃ¡ aleatoriamente entre los participantes. Â¡Mucha suerte!.\"\"\",\n",
    "    \"\"\"we play for yâ€™all ðŸ€â€¼ï¸ðŸ–¤ https://t.co/sd12vW93 #MambaMentality\"\"\",\n",
    "]\n",
    "\n",
    "for tweet in tweets:\n",
    "    print(word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "for text in texts:\n",
    "    print(tokenizer.tokenize(text))\n",
    "\n",
    "for tweet in tweets:\n",
    "    print(tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "for text in texts + tweets:\n",
    "    print(tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Veamos quÃ© tipo de tokenizaciÃ³n se prefiere cuando son humanos los que segmentan las palabras: el [corpus de Brown](https://en.wikipedia.org/wiki/Brown_Corpus) en inglÃ©s, o [Ancora](http://clic.ub.edu/corpus/es) en espaÃ±ol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "brown_sents = brown.tagged_sents(categories=\"news\")\n",
    "for sentence in brown_sents[:3]:\n",
    "    print([token for token, _tag in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import cess_esp\n",
    "\n",
    "ancora_sents = cess_esp.tagged_sents()\n",
    "for sentence in ancora_sents[:3]:\n",
    "    print([token for token, _tag in sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Representaciones discretas\n",
    "\n",
    "A partir de aquÃ­ vamos a asumir que tenemos solucionado el proceso de tokenizaciÃ³n e identificaciÃ³n de lo que es una palabra. Â¿CÃ³mo continuamos?\n",
    "\n",
    "La manera mÃ¡s sencilla de representar una palabra es una cadena, es decir, como una secuencia ordenada de caracteres. Esto es cÃ³modo, pero implica dos cosas:\n",
    "\n",
    "- La cantidad de memoria que ocupa cada cada palabra varÃ­a en funciÃ³n de la longitud :-/\n",
    "\n",
    "- Comprobar si dos palabras son idÃ©nticas es un proceso lento :-(\n",
    "\n",
    "Otra opciÃ³n alternativa consiste en representar las palabras como nÃºmeros enteros, de manera que a cada palabra se le asigna de manera mÃ¡s o menos arbitraria un nÃºmero entero positivo.\n",
    "\n",
    "![](img/words-indexes.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\"comp.windows.x\", \"rec.sport.baseball\", \"sci.space\", \"talk.religion.misc\"]\n",
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset=\"train\", categories=categories, remove=remove\n",
    ")\n",
    "\n",
    "newsgroups_train.filenames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "for doc in newsgroups_train.data[:3]:\n",
    "    print(doc[:300])\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "for doc in newsgroups_train.data[-3:]:\n",
    "    print(doc[:300])\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "tweets_tokens = []\n",
    "tweets_tokens.extend([tokenize(tweet) for tweet in tweets][0])\n",
    "print(tweets_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "tokens_int = label_encoder.fit_transform(tweets_tokens)\n",
    "\n",
    "token2int = dict(zip(tweets_tokens, tokens_int))\n",
    "print(token2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Este tipo de representaciÃ³n se caracteriza porque:\n",
    "\n",
    "- Todos las palabras ocupan la cantidad de memoria.\n",
    "\n",
    "- Comprobar si dos cadenas son la misma palabra es rÃ¡pido :-)\n",
    "\n",
    "- Estos identificadores arbitrarios no significan nada :-(\n",
    "\n",
    "- No hay manera de relacionar palabras similares atendiendo a su identificador :-("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# aplanamos los tokens_int\n",
    "tokens_int = tokens_int.reshape(len(tokens_int), 1)\n",
    "onehot_tokens = onehot_encoder.fit_transform(tokens_int)\n",
    "\n",
    "print(onehot_tokens)\n",
    "\n",
    "print(onehot_tokens[token2int[\"suerte\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "onehot_tokens = to_categorical(tokens_int)\n",
    "\n",
    "print(onehot_tokens)\n",
    "print(onehot_tokens[token2int[\"suerte\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizar colecciones de documentos es una prÃ¡ctica habitual. Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "vectors = vectorizer.fit_transform(\n",
    "    newsgroups_train.data\n",
    ").todense()  # (documents, vocab)\n",
    "\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Hemos convertido la collecciÃ³n de documentos en una matriz de datos donde los documentos se representan como vectores de enteros.\n",
    "\n",
    "![](img/vectorized-docs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(vocab[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(vocab[20000:20050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(vocab[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Palabras como vectores\n",
    "\n",
    "La verdad es que hoy en dÃ­a nunca se representamos las palabras como elementos discretos principalmente por un motivo: muchas veces no necesitaremos comprobar no si dos palabras son iguales, como mencionÃ¡bamos antes, pero sÃ­ si dos palabras son *similares*. Calcular la **similitud** entre dos pares de palabras/oraciones/documentos es crucial para muchas tareas de NLP.\n",
    "\n",
    "### Â¿QuÃ© significa **similitud**?\n",
    "\n",
    "La idea de similitud implica que dos palabras tienen algÃºn tipo de relaciÃ³n desde el punto de vista del significado, no necesariamente de sinonimia. Que dos palabras sean similares no implica que sean sinÃ³nimas e intercambiables en cualquier situaciÃ³n. \n",
    "\n",
    "- Dos adjetivos aparentemente contrarios (p. ej. *blanco* y *negro*) son similares porque se pueden aplicar a los mismos objetos. \n",
    "- Un tÃ©rmino general y otro mÃ¡s especÃ­fico (p. ej. *perro* y *caniche*) son similares tambiÃ©n. \n",
    "- Una parte constitutiva y el todo (p. ej. *dedo* y *mano*) son similares.\n",
    "\n",
    "HistÃ³ricamente, ha habido distintos intentos de codificar de manera explÃ­cita estas relaciones de similitud a mano. Desde el punto de vista SemÃ¡ntica, el ejemplo mÃ¡s famoso es [Wordnet](https://wordnet.princeton.edu/), una base de datos lÃ©xica que almacena palabras, sus significados y las relaciones semÃ¡nticas que se establecen entre ellas de manera jerÃ¡rquica. Otras partes de la lingÃ¼Ã­sitica, como la Sintaxis, estudia la estructura del lenguaje y agrupa las palabras similares bajo clases de palabras o categorÃ­as como *nombre*, *verbo*, *adjetivo*, etc. \n",
    "\n",
    "MÃ¡s rencientemente, se ha desarrollado una tendencia (hasta convertirse en dominante) consistente en extraer este tipo de relaciones de manera automÃ¡tica, a travÃ©s del procesamiento de ingentes colecciones de textos y el anÃ¡lisis de cada palabra en su contexto de apariciÃ³n.\n",
    "\n",
    "A partir de cualquiera de estas dos vertientes, o mÃ¡s bien, combinando ambas, podemos llegar a la idea de representar una palabra como un vector. Y podemos elegir la dimensionalidad que mejor se ajuste a nuestros intereses:\n",
    "\n",
    "\n",
    "- â€¢ Each word type may be given its own dimension, and assigned 1 in that dimension (while all other\n",
    "words get 0 in that dimension). Using dimensions only in this way, and no other, is essentially\n",
    "equivalent to integerizing the words; it is known as a â€œone hotâ€ representation, because each word\n",
    "typeâ€™s vector has a single 1 (â€œhotâ€) and is otherwise 0.\n",
    "â€¢ For a collection of word types that belong to a known class (e.g., days of the week), we can use a\n",
    "dimension that is given binary values. Word types that are members of the class get assigned 1 in this\n",
    "dimension, and other words get 0.\n",
    "â€¢ For word types that are variants of the same underlying root, we can similarly use a dimension to\n",
    "place them in a class. For example, in this dimension, know, known, knew, and knows would all get\n",
    "assigned 1, and words that are not forms of know get 0.\n",
    "â€¢ More loosely, we can use surface attributes to â€œtie togetherâ€ word types that look similar; examples\n",
    "include capitalization patterns, lengths, and the presence of a digit.\n",
    "â€¢ If word typesâ€™ meanings can be mapped to magnitudes, we might allocate dimensions to try to capture\n",
    "these. For example, in a dimension we choose to associate with â€œtypical weightâ€ elephant might get\n",
    "12,000 while cat might get 9. Of course, itâ€™s not entirely clear what value to give purple or throw in\n",
    "this dimension.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Cuando tenemos un mapeo como este entre palabras y entereos podemos representar cada palabra como un vector de $n$ dimensiones, donde $n$ es el tamaÃ±o de vocabulario que manejamos. Estos vectores contendrÃ¡n muchos $0$ y un Ãºnico $1$, en la posiciÃ³n que coincida con el Ã­ndice de la palabra en el vocabulario.\n",
    "\n",
    "![](img/one-hot-vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Palabras como vectores distribucionales\n",
    "\n",
    "> â€œYou shall know a word by the company it keeps.â€\n",
    "> â€” John R. Firth (1957)\n",
    ">\n",
    ">â€œThe meaning of a word is its use in the language (â€¦) One cannot guess how a word functions. One has to look at its use, and learn from that.â€\n",
    ">â€” Ludwig Wittgenstein (1953)\n",
    "\n",
    "\n",
    "La idea de que podemos analizar el uso de las palabras para deducir sus significado es una idea fundamental en semÃ¡ntica distribucional: la hipÃ³tesis distribucional. \n",
    "\n",
    "Esta idea inspira muchos algoritmos para aprender repesentaciones numÃ©ricas de las palanbras --> *word embeddings*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Palabras como vectores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
